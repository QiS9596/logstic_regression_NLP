{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "AU 19 CSE 5539-0010 \"Social Media and Text Analysis\" Homework #2  \n",
    "Wei Xu, The Ohio State University   \n",
    "\n",
    "In this assignment, we will walk you through the process of :\n",
    "\n",
    "- implementing logistic regression from scratch \n",
    "- and applying it to a real-world problem that predicts whether a student will be admitted to a university. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "IMPORTANG: In this assignment, except Numpy and Matplotlib, no other external Python packages are allowed. Scipy is used in gradient checking, though, it is not allowed elsewhere. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the Ohio State University's Code of Student Conduct, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(double click on this block and type your name here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Numpy and Matplotlib [Code provided - do not change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Check what version of Python is running\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have Numpy installed for the right version of Python. Most likely, you are using Python 3.6 in this Jupyter Notebook; then you may [install Numpy accordingly](https://stackoverflow.com/questions/37933978/install-numpy-in-python-3-4-4-and-linux). For example, installng Numpy via pip by using the command line \"sudo python3.6 -m pip install numpy\". If failed, you may need to update pip first by \"python3.6 -m pip install --upgrade pip\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# reload external python modules;\n",
    "# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualizing the Data  [Code provided - no need to change]\n",
    "\n",
    "The provided dataset contains applicants' scores on two exams and the admission decisons. \n",
    "\n",
    "[Matplotlib](http://matplotlib.org/users/pyplot_tutorial.html) is a Python package for data visualization. Suppose you are using Python 3.6, you can install by first use command line \"brew install freetype\", then \"sudo python3.6 -m pip install matplotlib\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "# of training examples =  100\n# of features =  2\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the dataset\n",
    "data = np.loadtxt('hw2_data.txt', delimiter=',')\n",
    "\n",
    "train_X = data[:, 0:2]\n",
    "train_y = data[:, 2]\n",
    "\n",
    "# Get the number of training examples and the number of features\n",
    "m_samples, n_features = train_X.shape\n",
    "print (\"# of training examples = \", m_samples)\n",
    "print (\"# of features = \", n_features)\n",
    "\n",
    "pos = np.where(train_y == 1)\n",
    "neg = np.where(train_y == 0)\n",
    "plt.scatter(train_X[pos, 0], train_X[pos, 1], marker='o', c='b')\n",
    "plt.scatter(train_X[neg, 0], train_X[neg, 1], marker='x', c='r')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.legend(['Not Admitted', 'Admitted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost Function [5 points]\n",
    "You're going to first implement the sigmoid function, then the cost function for (binary) logistic regression. \n",
    "\n",
    "The sigmoid function is defined as $sigmoid(\\mathbf{z}) = \\frac{1}{1+{e^{-\\mathbf{z}}}}$.\n",
    "\n",
    "Note that, you are asked to use the _numpy_ package for vector and matrix operations in order to ensure the __efficiency of the code__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "=== For autograder ===\n[[0.73105858 0.88079708]\n [0.26894142 0.11920292]]\n0.5\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE: be careful of the potential underflow or overflow here\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Check your sigmoid implementation\n",
    "z = np.array([[1, 2], [-1, -2]])\n",
    "f = sigmoid(z)\n",
    "print (\"=== For autograder ===\")\n",
    "print (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(theta, X, y):\n",
    "    \"\"\" The cost function for logistic regression \"\"\"\n",
    "    #####################################################################################\n",
    "    # Compute the cost given the current parameter theta on the training data set (X, y)#\n",
    "    #####################################################################################\n",
    "     \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Check your cost function implementation\n",
    "\n",
    "t_X = np.array([[1, 2], [-1, -2]])\n",
    "t_y = np.array([0, 1])\n",
    "t_theta1 = np.array([-10, 10])\n",
    "t_theta2 = np.array([10, -10])\n",
    "t_c1 = cost_function(t_theta1, t_X, t_y)\n",
    "t_c2 = cost_function(t_theta2, t_X, t_y)\n",
    "print (\"=== For autograder ===\")\n",
    "print (t_c1)\n",
    "print (t_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Computation [5 points]\n",
    "\n",
    "Implement the gradient computations for logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(theta, X, y):\n",
    "    \"\"\" The gradient update for logistic regression\"\"\"\n",
    "    ###############################\n",
    "    # Compute the gradient update #\n",
    "    ###############################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    grad = grad / m_samples  \n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Check your gradient computation implementation\n",
    "t_X = np.array([[1, 2], [-1, -2]])\n",
    "t_y = np.array([0, 1])\n",
    "t_theta1 = np.array([-10, 10])\n",
    "t_theta2 = np.array([10, -10])\n",
    "t_g1 = gradient_update(t_theta1, t_X, t_y)\n",
    "t_g2 = gradient_update(t_theta2, t_X, t_y)\n",
    "print (\"=== For autograder ===\")\n",
    "print (t_g1)\n",
    "print (t_g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Checking [Code provided. Bonus 5 points if implemented from scratch]\n",
    "You can use the code provided below to check the gradient of your logistic regression functions. Alternatively, you can implementing the gradient checking from scratch by yourself (bonus 10 points). \n",
    "\n",
    "[Gradient checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) is an important technique for debugging the gradient computation. Logistic regression is a relatively simple algorithm where it is straightforward to derive and implement its cost function and gradient computation. For more complex models, the gradient computaitn can be notoriously difficulty to debug and get right. Sometimes a subtly buggy implementation will manage to learn something that can look surprisingly reasonable, while performing less well than a correct implementation. Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your gradient computation implementation\n",
    "t_samples, t_features = 100, 10\n",
    "t_X = np.random.randn(t_samples, t_features)\n",
    "t_y = np.random.randint(2, size=t_samples) \n",
    "t_theta = np.random.randn(t_features)\n",
    "\n",
    "from scipy import optimize\n",
    "print (\"=== For autograder ===\")\n",
    "print('Output of check_grad: %s' % optimize.check_grad(cost_function, gradient_update, t_theta, t_X, t_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Descent  and Decision Boundary  [10 points]\n",
    "\n",
    "Implement the batch gradient decent algorithm for logistic regression. For every 'print_iterations' number of iterations, also visualize the decision boundary and obeserve how it changes during the training.\n",
    "\n",
    "Note that, you will need to carefully choose the learning rate and the total number of iterations, especially given that the starter code does not include feature scaling (e.g., scale each feature by its maximum absolute value to convert feature value to [-1,1] range -- in order to make this homework simple and easier for you to write code to visualize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(theta, X, y, alpha, max_iterations, print_iterations):\n",
    "    \"\"\" Batch gradient descent algorithm \"\"\"\n",
    "    #################################################################\n",
    "    # Update the parameter 'theta' iteratively to minimize the cost #\n",
    "    # Also visualize the decision boundary during learning          #\n",
    "    #################################################################\n",
    " \n",
    "    alpha *= m_samples\n",
    "    iteration = 0\n",
    "    \n",
    "    ### YOUR CODE HERE: \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    \n",
    "    \n",
    "    while(iteration < max_iterations):\n",
    "        iteration += 1\n",
    "        \n",
    "        ### YOUR CODE HERE: simultaneous update of partial gradients\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        \n",
    "        # For every 25 iterations\n",
    "        if iteration % print_iterations == 0 or iteration == 1:\n",
    "            cost = 0\n",
    "            \n",
    "            ### YOUR CODE HERE: calculate the cost\n",
    "            ### IMPORTANT: The cost function is guaranteed to decrease after \n",
    "            ## every iteration of the gradient descent algorithm.\n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "            ### END YOUR CODE\n",
    "            \n",
    "            \n",
    "            print (\"[ Iteration\", iteration, \"]\", \"cost =\", cost)\n",
    "            plt.rcParams['figure.figsize'] = (5, 4)\n",
    "            plt.xlim([20,110])\n",
    "            plt.ylim([20,110])\n",
    "            \n",
    "            pos = np.where(y == 1)\n",
    "            neg = np.where(y == 0)\n",
    "            \n",
    "            plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\n",
    "            plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\n",
    "            plt.xlabel('Exam 1 score')\n",
    "            plt.ylabel('Exam 2 score')\n",
    "            plt.legend(['Not Admitted', 'Admitted'])\n",
    "            t = np.arange(10, 100, 0.1)\n",
    "            \n",
    "            \n",
    "            ### YOUR CODE HERE: plot the decision boundary\n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "            ### END YOUR CODE \n",
    "            \n",
    "            \n",
    "            plt.show()\n",
    "               \n",
    "    return theta\n",
    "\n",
    "\n",
    "### YOUR CODE HERE: initialize the parameters 'theta' to random values; \n",
    "### And set up learning rate, number of max iterations, number of iterations for printing intermedia outputs\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "### END YOUR CODE\n",
    "\n",
    "\n",
    "learned_theta = gradient_descent(initial_theta, train_X, train_y, alpha_test, max_iter, print_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predicting [5 points]\n",
    "Now that you learned the parameters of the model, you can use the model to prdict whether a particular student will be admited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \"\"\" Predict whether the label is 0 or 1 using learned logistic regression parameters \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE: \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    \n",
    "    ## convert an array of booleans 'predicted_labels' into an array of 0 or 1 intergers\n",
    "    return probabilities, 1*predicted_labels \n",
    "\n",
    "# Check your predication function implementation\n",
    "t_X1 = np.array([[90, 90]])\n",
    "t_X2 = np.array([[50, 60]])\n",
    "t_X3 = np.array([[10, 50]])\n",
    "print (\"=== For autograder ===\")\n",
    "print (predict(learned_theta, t_X1))\n",
    "print (predict(learned_theta, t_X2))\n",
    "print (predict(learned_theta, t_X3))\n",
    "\n",
    "# Computer accuracy on the training dateset \n",
    "t_prob, t_label = predict(learned_theta, train_X)\n",
    "t_precision = t_label[np.where(t_label == train_y)].size / float(train_y.size) * 100\n",
    "print (\"=== For autograder ===\")\n",
    "print('Accuracy on the training set: %s%%' % round(t_precision,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Submit Your Homework\n",
    "This is the end. Congratulations! \n",
    "\n",
    "Now, follow the steps below to submit your homework in [Carmen](https://carmen.osu.edu/):\n",
    "\n",
    "1. rename this ipynb file to 'hw2_yourdotid.ipynb' \n",
    "2. click on the menu 'File' --> 'Download as' --> 'Python (.py)'\n",
    "3. pack both the above 'hw2_yourdotid.ipynb' file and the 'hw2_yourdotid.py' file into a zip file 'hw2_yourdotid.zip'\n",
    "4. upload the zip file 'hw2_yourdotid.zip' in Carmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}